# Architecture Rationale, Capacity Assumptions, and Evolution Checkpoints — Modula

## Purpose

This document records **explicit architectural reasoning and capacity assumptions** behind Modula.
It serves two goals:

1. **Academic defense** — explaining *why* specific architectural choices were made
2. **Future reference** — helping future maintainers (including future self) understand original constraints and intent

This document does not define business logic; it **justifies it**.

---

## 1. Product Reality and Usage Model

Modula is a **Point-of-Sale system**, not a mass consumer platform.

Key characteristics:
- Usage is **human-paced**, not machine-paced
- Requests are generated by staff actions (cashiers, managers)
- Traffic is bounded by:
  - number of branches
  - number of registers
  - business hours
- Peak usage occurs during short daily windows

This fundamentally shapes capacity and architectural decisions.

---

## 2. Capacity Assumptions (Explicit)

### Tenant Scale Assumption
- Initial: 1–10 tenants
- Growth target: 50–500 tenants (Cambodian market realism)
- Each tenant:
  - 1–3 branches
  - 1–3 registers per branch

### Traffic Assumption
- Average register throughput:
  - ~1 order every 20–60 seconds
- Orders generate a small number of API calls
- Expected peak load:
  - hundreds to low thousands of requests per second
- **Not designed for 100K+ RPS** (not required for POS domain)

These assumptions justify avoiding hyperscale architectures.

---

## 3. Why Modular Monolith Was Chosen

### Chosen: Modular Monolith
### Not chosen: Microservices-first architecture

Rationale:
- Core complexity lies in **business correctness**, not network scale
- Money, inventory, and cash session flows require **strong consistency**
- Single-database transactions simplify invariant enforcement
- Faster iteration and lower operational overhead for early-stage product

Modula uses:
- Clear domain boundaries
- Explicit cross-module process orchestration
- Internal modularization without distributed systems overhead

This choice remains reversible if scale exceeds assumptions.

---

## 4. Why Ledger-Based Inventory Was Chosen

### Problem With CRUD Inventory
- Overwrites historical truth
- Makes reconciliation difficult
- Unsafe under retries and offline operation
- Poor auditability

### Chosen: Ledger + Projections
- Append-only inventory movements as source of truth
- Derived projections for fast reads
- Safe correction via compensating entries
- Rebuildable state for operational recovery

This model supports:
- Offline-first behavior
- Multi-year data growth
- Audit requirements
- Performance stability

---

## 5. Ledger Growth Is a Time Problem, Not a Traffic Problem

Inventory ledgers grow because:
- the system runs for years
- operations are recorded permanently

They do not grow because of extreme request volume.

Therefore, performance strategy focuses on:
- projections (BranchStock)
- rollups
- partitioning
- rebuild capability

Not on:
- extreme load balancing
- distributed consensus
- blockchain-style infrastructure

---

## 6. Background Jobs as Maintenance, Not Truth

Background jobs exist to:
- improve read performance
- maintain projections
- support recovery and integrity checks

They do **not** define correctness.

Correctness always comes from:
- domain invariants
- append-only ledgers
- idempotent commands

This separation reduces operational risk.

---

## 7. Load Balancing Compatibility

The architecture supports horizontal scaling because:
- application layer is stateless
- invariants are enforced at database level
- idempotency protects against duplicate requests
- projections prevent expensive reads

Load balancing is possible without architectural change, but not prematurely optimized.

---

## 8. Explicit Non-Goals (At This Stage)

- Hyperscale consumer traffic
- Microservice sprawl
- Blockchain-based ledgers
- Warehouse-grade inventory allocation
- Real-time global analytics

These may be revisited only if assumptions change.

---

## 9. Evolution Checkpoints (If Modula Succeeds)

If growth exceeds assumptions, possible next steps include:
- Tenant-based database partitioning
- Read replicas for projections
- Dedicated background worker services
- Service extraction for high-isolation domains

These are **evolution paths**, not initial requirements.

---

## 10. Platform Provider Decision (Locked for March)

### Decision
For March delivery and initial real-café deployment, Modula will use:
- **Supabase** for:
  - database hosting (managed Postgres)
  - authentication (Supabase Auth)
- **Cloudflare R2** for:
  - file storage (user-uploaded images)

**Decision date:** 2026-02-06

### Why This Fits Modula
- Modula’s correctness depends on **transactional invariants** (cash session, sale finalization, inventory ledger). Postgres supports this naturally.
- Supabase Auth supports a **global person identity** (phone-based login) which matches our SaaS model: one person can belong to multiple tenants.
- Cloudflare R2 provides low-friction object storage during the capstone phase, keeping operational cost low while we have no revenue.
- The domain model remains clean:
  - Supabase Auth answers “who is this person?” (authentication)
  - Modula domains answer “which tenant/branch can they operate in?” (authorization via memberships/assignments)

### Design Mapping (No Domain Drift)
- `auth_account_id` (our stable identity key across the KnowledgeBase) maps to Supabase `auth.users.id`.
- Tenant Membership / Staff Profile / Access Control remain **our own tables and rules** (Supabase does not model our multi-tenant org structure).
- Owner-provisioned onboarding remains valid:
  - admins/owners provision **membership** by phone
  - staff own credentials via OTP/self-service flows
  - no admin ever sets/knows a staff password (even if a user record is provisioned server-side)
- User-uploaded images (tenant logo, staff avatar, menu item images, stock item images) are stored in Cloudflare R2.
  - The database stores an object reference (path/key) + metadata; clients do not treat public URLs as stable identity.
  - Object keys should be tenant-scoped (example prefix: `tenants/{tenant_id}/...`) to keep multi-tenant boundaries explicit.

### Security / Operational Constraints
- Supabase service role credentials are **server-only** (never shipped to clients).
- Multi-tenant isolation is enforced by Modula’s authorization model (and optionally database RLS), not by “trusting the client session context”.
- Storage access should follow the same rule:
  - default to a private R2 bucket + signed URLs so one tenant cannot read another tenant’s uploads.
  - R2 credentials are server-only; clients upload/download only via signed URLs issued after Access Control checks.

---

## 11. Final Note to Future Self

These decisions were made with:
- limited market size
- real café operations
- academic accountability
- long-term maintainability in mind

If changing them, first ask:
> “Which assumption is no longer true?”

Architecture should evolve from **reality**, not fear.
